{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Output Notebook\n",
    "\n",
    "<img style=\"float:center;\" src=\"https://arcticexpansion.vse.gmu.edu/sites/arcticexpansion.vsnet.gmu.edu/files/images/header5d2.png\" width=600px>\n",
    "\n",
    "### ADCIRC-SWAN Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings;warnings.filterwarnings(\"ignore\")\n",
    "import netCDF4 as nc4;        import pandas as pd\n",
    "import requests;              import json;\n",
    "import matplotlib as mpl;     import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as tri; import pathlib as pl\n",
    "\n",
    "import numpy as np;           import xarray as xr\n",
    "import skill_metrics as sm;   import geopandas as gpd\n",
    "\n",
    "import wse\n",
    "import dask.array as da\n",
    "from dask.diagnostics import ProgressBar\n",
    "from shapely import Polygon,Point,MultiPoint,LineString,MultiLineString;import shapely.vectorized\n",
    "from sklearn.neighbors import BallTree\n",
    "from scipy.stats import linregress\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import sparse\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds, year, keep_idx=None):\n",
    "    \"\"\"\n",
    "    For one year’s dataset:\n",
    "    - Use the file’s own time values (decoded by xarray).\n",
    "    - Keep only timestamps within the calendar year (drops spin‑up & overrun).\n",
    "    \"\"\"\n",
    "    # Convert the decoded time coordinate to pandas DatetimeIndex\n",
    "    time = pd.to_datetime(ds.time.values)\n",
    "    #year = time[0].year\n",
    "\n",
    "    # Build the calendar‐year mask\n",
    "    start = pd.to_datetime(f\"{year}-01-01T00:00:00\")\n",
    "    end   = pd.to_datetime(f\"{year}-12-31T23:00:00\")\n",
    "    mask  = (time >= start) & (time <= end)\n",
    "    ds.isel(time=mask)\n",
    "    if keep_idx is not None:\n",
    "        ds = ds.isel(node=keep_idx)\n",
    "    \n",
    "    return ds\n",
    "def open_zeta_dataset(root_path, years, chunks={\"time\":1000, \"node\":20000}):\n",
    "    \"\"\"\n",
    "    Open each year’s CF‑ified file with mask_and_scale=False (no NaNs),\n",
    "    preprocess it, then concat along time.\n",
    "    \"\"\"\n",
    "    ds_list = []\n",
    "    for year in years:\n",
    "        path = pl.Path(root_path) / str(year) / \"fort.63.cf.nc\"\n",
    "        # disable automatic masking of the fill_value\n",
    "        ds = xr.open_dataset(path,\n",
    "                             engine=\"netcdf4\",\n",
    "                             chunks=chunks,\n",
    "                             mask_and_scale=False)\n",
    "        ds = preprocess(ds, year)\n",
    "        ds_list.append(ds)\n",
    "    return xr.concat(ds_list, dim=\"time\")\n",
    "def build_adjacency_from_sample(path):\n",
    "    \"\"\"\n",
    "    Build sparse node‐to‐node adjacency from one sample CF file.\n",
    "    \"\"\"\n",
    "    ds0 = xr.open_dataset(path, engine=\"netcdf4\")\n",
    "    elems = ds0[\"face_node_connectivity\"].values - 1  # shape (nele, 3)\n",
    "    ds0.close()\n",
    "\n",
    "    n = elems.max() + 1\n",
    "    row = np.hstack([elems[:,0], elems[:,1],\n",
    "                     elems[:,1], elems[:,2],\n",
    "                     elems[:,2], elems[:,0]])\n",
    "    col = np.hstack([elems[:,1], elems[:,0],\n",
    "                     elems[:,2], elems[:,1],\n",
    "                     elems[:,0], elems[:,2]])\n",
    "    data = np.ones_like(row, dtype=int)\n",
    "    return sparse.coo_matrix((data, (row, col)), shape=(n, n)).tocsr()\n",
    "\n",
    "def compute_3h_max(ds):\n",
    "    \"\"\"\n",
    "    Compute rolling 3‑hour max on ds.zeta, keeping only the first\n",
    "    two “incomplete” hours as NaN and then slicing them off.\n",
    "    \"\"\"\n",
    "    z = ds.zeta\n",
    "    # perform a 3‑step window max, requiring all 3 points\n",
    "    surge3 = z.rolling(time=3, center=False, min_periods=3).max()\n",
    "    # drop the first two timestamps, which will be NaN\n",
    "    return surge3.isel(time=slice(2, None))\n",
    "def extract_thresholds(surge3):\n",
    "    # Compute in‑place on Dask arrays; returns two DataArrays\n",
    "    p95_da  = surge3.quantile(0.95,  dim=\"time\", skipna=True)\n",
    "    p999_da = surge3.quantile(0.999, dim=\"time\", skipna=True)\n",
    "    # Now pull the resulting 1D arrays into memory\n",
    "    th1 = p95_da.data.compute()\n",
    "    th2 = p999_da.data.compute()\n",
    "    return th1, th2\n",
    "\n",
    "def detect_node_peaks(surge3, th2, chunk_size=1000):\n",
    "    \"\"\"\n",
    "    Stream through nodes in chunks, loading only chunk_size columns at once.\n",
    "    Returns dict: global_node_index -> array of peak time‐indices.\n",
    "    \"\"\"\n",
    "    from scipy.signal import find_peaks\n",
    "\n",
    "    n_time = surge3.sizes[\"time\"]\n",
    "    n_node = surge3.sizes[\"node\"]\n",
    "    peaks = {}\n",
    "\n",
    "    for i in range(0, n_node, chunk_size):\n",
    "        j = min(n_node, i + chunk_size)\n",
    "        # load a small block: shape = (time, j-i)\n",
    "        block = surge3.isel(node=slice(i, j)).values\n",
    "        th2_block = th2[i:j]\n",
    "\n",
    "        for local_idx in range(block.shape[1]):\n",
    "            idxs, _ = find_peaks(\n",
    "                block[:, local_idx],\n",
    "                height=th2_block[local_idx],\n",
    "                distance=24\n",
    "            )\n",
    "            if idxs.size:\n",
    "                peaks[i + local_idx] = idxs\n",
    "\n",
    "    return peaks\n",
    "\n",
    "\n",
    "def find_spatial_footprints(surge3, th1, th2, adjacency):\n",
    "    \"\"\"\n",
    "    Iterate over time steps, loading one time slice at a time.\n",
    "    Returns dict: time_index -> label array for all nodes.\n",
    "    \"\"\"\n",
    "    from scipy.sparse.csgraph import connected_components\n",
    "\n",
    "    n_node = surge3.sizes[\"node\"]\n",
    "    footprints = {}\n",
    "\n",
    "    for t in range(surge3.sizes[\"time\"]):\n",
    "        # load only one timestep: shape = (node,)\n",
    "        z_t = surge3.isel(time=t).values\n",
    "        # quick check if any node exceeds the high threshold\n",
    "        if not (z_t >= th2).any():\n",
    "            continue\n",
    "\n",
    "        # mask at the lower threshold and cluster\n",
    "        mask1 = z_t >= th1\n",
    "        nodes = np.nonzero(mask1)[0]\n",
    "        sub_adj = adjacency[nodes][:, nodes]\n",
    "        labels, _ = connected_components(sub_adj, directed=False)\n",
    "\n",
    "        full_labels = -1 * np.ones(n_node, dtype=int)\n",
    "        full_labels[nodes] = labels\n",
    "        footprints[t] = full_labels\n",
    "\n",
    "    return footprints\n",
    "\n",
    "def cluster_events(feature_matrix, n_clusters=8):\n",
    "    \"\"\"PCA + k‑means clustering of event features.\"\"\"\n",
    "    scores = PCA(n_components=6).fit_transform(feature_matrix)\n",
    "    return KMeans(n_clusters=n_clusters, random_state=0).fit_predict(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/scratch/tmiesse/project/data4spatial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time span: 2022-12-15T00:00:00.000000000 to 2024-12-29T23:00:00.000000000\n",
      "Working on full mesh with 861878 nodes\n"
     ]
    }
   ],
   "source": [
    "YEARS = range(2023, 2025)\n",
    "MAX_DEPTH = 20.0\n",
    "\n",
    "sample_path = f\"{root}/2023/fort.63.cf.nc\"\n",
    "adj = build_adjacency_from_sample(sample_path)\n",
    "ds = open_zeta_dataset(root, YEARS)\n",
    "ds     = ds.chunk({\"time\":1000, \"node\":20000})\n",
    "print(\"Time span:\", ds.time.values[0], \"to\", ds.time.values[-1])\n",
    "print(\"Working on full mesh with\", ds.sizes[\"node\"], \"nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Compute rolling 3h max\n",
    "surge3 = compute_3h_max(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Extract thresholds\n",
    "th1, th2 = extract_thresholds(surge3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Detect peaks & footprints\n",
    "peaks = detect_node_peaks(surge3, th2)\n",
    "fps   = find_spatial_footprints(surge3, th1, th2, adj)\n",
    "print(f\"Found peaks on {len(peaks)} nodes, {len(fps)} time slices with footprints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
