{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Output Notebook\n",
    "\n",
    "<img style=\"float:center;\" src=\"https://arcticexpansion.vse.gmu.edu/sites/arcticexpansion.vsnet.gmu.edu/files/images/header5d2.png\" width=600px>\n",
    "\n",
    "### ADCIRC-SWAN Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc4;        import pandas as pd\n",
    "import pathlib as pl;         import geopandas as gpd\n",
    "import numpy as np;           import xarray as xr\n",
    "import multiprocessing as mp; import datetime\n",
    "import os\n",
    "from shapely import Polygon,Point,MultiPoint,LineString,MultiLineString;import shapely.vectorized\n",
    "from sklearn.neighbors import BallTree\n",
    "from scipy.stats import linregress\n",
    "from collections import defaultdict\n",
    "import warnings;warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source $HOME/miniforge3/bin/activate\n",
    "\n",
    "salloc --ntasks=5 --nodes=1 --partition=normal --time=10:00:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for this exercise can be found here\n",
    "https://doi.org/10.17603/ds2-h0fw-2p96\n",
    "\n",
    "Download the swan_HS.63.nc from one of the 4 folders\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunk(chunk_df, chunk_id):\n",
    "    node_ids = chunk_df[\"node_ids\"].values.astype(int)\n",
    "    comm_names = chunk_df[\"name\"].astype(str).values\n",
    "    comm_lats = chunk_df.geometry.y.values\n",
    "    comm_lons = chunk_df.geometry.x.values\n",
    "    max_name_len = max(len(name) for name in comm_names)\n",
    "\n",
    "    file_var_map = {\n",
    "        \"zeta\": \"fort.63.nc\", \"depth\": \"fort.63.nc\", \"u-vel\": \"fort.64.nc\", \"v-vel\": \"fort.64.nc\",\n",
    "        \"pressure\": \"fort.73.nc\", \"windx\": \"fort.74.nc\", \"windy\": \"fort.74.nc\",\n",
    "        \"iceaf\": \"fort.93.nc\", \"swan_HS\": \"swan_HS.63.nc\",\n",
    "        \"swan_TPS\": \"swan_TPS.63.nc\", \"swan_DIR\": \"swan_DIR.63.nc\"\n",
    "    }\n",
    "    file_to_vars = defaultdict(list)\n",
    "    for var, fname in file_var_map.items():\n",
    "        file_to_vars[fname].append(var)\n",
    "\n",
    "    # Get time from fort.63.nc\n",
    "    fort63 = OUTPUT_DIR / \"swan_HS.63.nc\"\n",
    "    with nc4.Dataset(fort63) as ds:\n",
    "        base_time = pd.to_datetime(ds.variables[\"time\"].base_date)\n",
    "        nt = ds.dimensions[\"time\"].size\n",
    "        time_all_dt = pd.date_range(start=base_time, periods=nt, freq=\"1H\").to_numpy(\"datetime64[ns]\")\n",
    "        time_all = (time_all_dt - np.datetime64(\"1970-01-01T00:00:00Z\")) // np.timedelta64(1, \"s\")\n",
    "\n",
    "    # Filter to year only\n",
    "    start = np.datetime64(f\"{YEAR}-01-01T00:00:00\")\n",
    "    end = np.datetime64(f\"{YEAR}-12-31T23:00:00\")\n",
    "    mask = (time_all_dt >= start) & (time_all_dt <= end)\n",
    "    time_all = time_all[mask]\n",
    "\n",
    "    # Extract data\n",
    "    data_by_var = {var: None for var in file_var_map}\n",
    "    for fname, var_list in file_to_vars.items():\n",
    "        file_path = OUTPUT_DIR / fname\n",
    "        if not file_path.exists(): continue\n",
    "\n",
    "        with nc4.Dataset(file_path, \"r\") as ds:\n",
    "            for var in var_list:\n",
    "                nc_var = var if var in ds.variables else var.replace(\"-\", \"_\")\n",
    "                if nc_var not in ds.variables: continue\n",
    "                v = ds.variables[nc_var]\n",
    "                if v.ndim == 1:\n",
    "                    total_nodes = v.shape[0]\n",
    "                    valid_indices = [i for i in node_ids if i < total_nodes]\n",
    "                    if len(valid_indices) < len(node_ids):\n",
    "                        print(f\"⚠️ Some node_ids are out of bounds for {file_path.name} (max index = {total_nodes-1})\")\n",
    "                    full_data = np.full((nt, len(node_ids)), np.nan, dtype=np.float32)\n",
    "                    node_data = v[valid_indices]\n",
    "                    for i, vi in enumerate(valid_indices):\n",
    "                        full_data[:, i] = node_data[i]  # broadcast across time\n",
    "                    data_by_var[var] = full_data[mask, :]\n",
    "                else:\n",
    "                    total_nodes = v.shape[1]\n",
    "                    valid_indices = [i for i in node_ids if i < total_nodes]\n",
    "                    if len(valid_indices) < len(node_ids):\n",
    "                        print(f\"⚠️ Some node_ids are out of bounds for {file_path.name} (max index = {total_nodes-1})\")\n",
    "                    data = np.full((nt, len(node_ids)), np.nan, dtype=np.float32)\n",
    "                    for t in range(nt):\n",
    "                        try:\n",
    "                            data[t, :len(valid_indices)] = v[t, valid_indices]\n",
    "                        except RuntimeError as e:\n",
    "                            print(f\"❌ Error reading variable '{var}' from file '{file_path}' at timestep {t}: {e}\")\n",
    "                            continue\n",
    "                    data_by_var[var] = data[mask, :]\n",
    "\n",
    "    # Write to NetCDF\n",
    "    out_path = PROCESSED_DIR / f\"extracted_outputs_{YEAR}_part{chunk_id:02d}.nc\"\n",
    "    nt_filtered = len(time_all)\n",
    "    with nc4.Dataset(out_path, \"w\", format=\"NETCDF4\") as ds_out:\n",
    "        ds_out.title = f\"Arctic Alaska Coastal Hazards Dataset [1979 - 2024] Data for {YEAR}\"\n",
    "        ds_out.institution = \"GMU Flood Hazards Research Lab\"\n",
    "        ds_out.source = \"ADCIRC-SWAN\"\n",
    "        ds_out.history = f\"Created on {datetime.datetime.now()} by Tyler Miesse\"\n",
    "        ds_out.Conventions = \"CF-1.8\"\n",
    "        ds_out.contact = \"tmiesse@gmu.edu\"\n",
    "        ds_out.author = \"Tyler W. Miesse, Andre de Souza de Lima, Martin Henke, Celso Ferreira, and Thomas Ravens\"\n",
    "        ds_out.acknowledgment = \"This research was supported by funding from the National Science Foundation (Award No. 1927785) .\"\n",
    "        ds_out.funding = \"NSF NNA Track 1: Arctic impacts and reverberations of expanding global maritime trade routes\"\n",
    "        ds_out.summary = (\n",
    "            \"This study utilizes ADCIRC+SWAN to simulate interactions between the ocean, land, sea ice, and atmosphere,\"\\\n",
    "            \" focusing on the period from 1979 to 2024 for Western to Northern Alaska coasts. Data from the European Centre\"\\\n",
    "            \" for Medium-Range Weather Forecasts Re-Analysis (ERA5), including sea ice concentration and atmospheric forcing\"\\\n",
    "            \" were utilized to support these simulations, which investigate annual conditions in the Alaskan Arctic. This is\" \\\n",
    "            \" dataset is extracted parameters for communities found in western to northern Alaska. For other areas in Alaska\" \\\n",
    "            \" not found in this dataset please check out the Raw_DATA.\"\n",
    "        )\n",
    "\n",
    "        ds_out.createDimension(\"time\", nt_filtered)\n",
    "        ds_out.createDimension(\"node\", len(node_ids))\n",
    "        ds_out.createDimension(\"name_strlen\", max_name_len)\n",
    "\n",
    "        tvar = ds_out.createVariable(\"time\", \"f8\", (\"time\",))\n",
    "        tvar[:] = time_all\n",
    "        tvar.units = \"seconds since 1970-01-01 00:00:00\"\n",
    "        tvar.calendar = \"standard\"\n",
    "        tvar.long_name = \"Time\"\n",
    "\n",
    "        lat = ds_out.createVariable(\"lat\", \"f4\", (\"node\",)); lat[:] = comm_lats; lat.long_name = \"Latitude\"\n",
    "        lon = ds_out.createVariable(\"lon\", \"f4\", (\"node\",)); lon[:] = comm_lons; lon.long_name = \"Longitude\"\n",
    "\n",
    "        name_array = np.array([list(n.ljust(max_name_len)) for n in comm_names], dtype=\"S1\")\n",
    "        name_var = ds_out.createVariable(\"community\", \"S1\", (\"node\", \"name_strlen\"))\n",
    "        name_var[:, :] = name_array\n",
    "        name_var.long_name = \"Community name\"\n",
    "\n",
    "        for var in data_by_var:\n",
    "            v = ds_out.createVariable(var, \"f4\", (\"time\", \"node\"), zlib=True, fill_value=np.nan)\n",
    "            if var == \"zeta\":\n",
    "                v.long_name = \"Water surface elevation above msl\"\n",
    "                v.units = \"m\"\n",
    "                v.standard_name = \"sea_surface_height_above_msl\"\n",
    "                v.description = \"Modeled water level relative to local mean sea level. Positive upwards.\"\n",
    "                v.valid_min = -50.0; v.valid_max = 50.0\n",
    "            elif var == \"u-vel\":\n",
    "                v.long_name = \"Eastward depth-averaged velocity\"\n",
    "                v.units = \"m/s\"\n",
    "                v.standard_name = \"eastward_sea_water_velocity\"\n",
    "                v.description = \"Eastward component of depth-averaged ocean current.\"\n",
    "                v.valid_min = -50.0; v.valid_max = 50.0\n",
    "            elif var == \"v-vel\":\n",
    "                v.long_name = \"Northward depth-averaged velocity\"\n",
    "                v.units = \"m/s\"\n",
    "                v.standard_name = \"northward_sea_water_velocity\"\n",
    "                v.description = \"Northward component of depth-averaged ocean current.\"\n",
    "                v.valid_min = -50.0; v.valid_max = 50.0\n",
    "            elif var == \"pressure\":\n",
    "                v.long_name = \"Atmospheric pressure at 10m\"\n",
    "                v.units = \"Pa\"\n",
    "                v.standard_name = \"air_pressure_at_sea_level\"\n",
    "                v.description = \"Surface atmospheric pressure at 10-meter height.\"\n",
    "                v.valid_min = 60000.0; v.valid_max = 130000.0\n",
    "            elif var == \"windx\":\n",
    "                v.long_name = \"Eastward 10m wind velocity\"\n",
    "                v.units = \"m/s\"\n",
    "                v.standard_name = \"eastward_wind\"\n",
    "                v.description = \"Eastward component of wind velocity at 10 meters.\"\n",
    "                v.valid_min = -50.0; v.valid_max = 50.0\n",
    "            elif var == \"windy\":\n",
    "                v.long_name = \"Northward 10m wind velocity\"\n",
    "                v.units = \"m/s\"\n",
    "                v.standard_name = \"northward_wind\"\n",
    "                v.description = \"Northward component of wind velocity at 10 meters.\"\n",
    "                v.valid_min = -50.0; v.valid_max = 50.0\n",
    "            elif var == \"iceaf\":\n",
    "                v.long_name = \"Sea ice area fraction\"\n",
    "                v.units = \"1\"\n",
    "                v.standard_name = \"sea_ice_area_fraction\"\n",
    "                v.description = \"Pecent fraction of ocean surface covered by sea ice (0 to 100).\"\n",
    "                v.valid_min = 0.0; v.valid_max = 100.0\n",
    "            elif var == \"swan_HS\":\n",
    "                v.long_name = \"Significant wave height\"\n",
    "                v.units = \"m\"\n",
    "                v.standard_name = \"significant_height_of_wind_and_swell_waves\"\n",
    "                v.description = \"Height of significant wind and swell waves.\"\n",
    "                v.valid_min = 0.0; v.valid_max = 30.0\n",
    "            elif var == \"swan_TPS\":\n",
    "                v.long_name = \"Peak spectral wave period\"\n",
    "                v.units = \"s\"\n",
    "                v.standard_name = \"wave_period_at_variance_spectral_density_maximum\"\n",
    "                v.description = \"Wave period at the peak of the variance density spectrum.\"\n",
    "                v.valid_min = 0.0; v.valid_max = 30.0\n",
    "            elif var == \"swan_DIR\":\n",
    "                v.long_name = \"Mean wave direction\"\n",
    "                v.units = \"degrees\"\n",
    "                v.standard_name = \"direction_of_wind_waves\"\n",
    "                v.description = \"Direction from which waves are coming, measured clockwise from true north.\"\n",
    "                v.valid_min = 0.0; v.valid_max = 360.0\n",
    "            elif var == \"depth\":\n",
    "                v.long_name = \"Bathymetric depth at node\"\n",
    "                v.units = \"m\"\n",
    "                v.standard_name = \"sea_floor_depth_below_geoid\"\n",
    "                v.description = \"Depth of the ocean bottom relative to the geoid.\"\n",
    "                v.positive = \"down\"\n",
    "                v.valid_min = -1000.0; v.valid_max = 1000.0\n",
    "            if data_by_var[var] is not None:\n",
    "                v[:, :] = data_by_var[var]\n",
    "            else:\n",
    "                v[:, :] = np.full((nt_filtered, len(node_ids)), np.nan)\n",
    "\n",
    "    print(f\"✅ Finished chunk {chunk_id:02d} → {out_path.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished chunk 03 → extracted_outputs_2000_part03.nc✅ Finished chunk 04 → extracted_outputs_2000_part04.nc✅ Finished chunk 08 → extracted_outputs_2000_part08.nc✅ Finished chunk 05 → extracted_outputs_2000_part05.nc✅ Finished chunk 07 → extracted_outputs_2000_part07.nc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✅ Finished chunk 10 → extracted_outputs_2000_part10.nc\n",
      "✅ Finished chunk 12 → extracted_outputs_2000_part12.nc\n",
      "✅ Finished chunk 11 → extracted_outputs_2000_part11.nc\n",
      "✅ Finished chunk 01 → extracted_outputs_2000_part01.nc\n",
      "✅ Finished chunk 09 → extracted_outputs_2000_part09.nc\n",
      "✅ Finished chunk 02 → extracted_outputs_2000_part02.nc\n",
      "✅ Finished chunk 06 → extracted_outputs_2000_part06.nc\n",
      "✅ All chunks complete.\n",
      "🔄 Merging chunks with xarray...\n",
      "✅ Merged NetCDF written to /scratch/tmiesse/project/data4trends/2000.nc and part files removed.\n",
      "✅ Finished chunk 01 → extracted_outputs_1999_part01.nc✅ Finished chunk 04 → extracted_outputs_1999_part04.nc✅ Finished chunk 05 → extracted_outputs_1999_part05.nc✅ Finished chunk 11 → extracted_outputs_1999_part11.nc✅ Finished chunk 12 → extracted_outputs_1999_part12.nc✅ Finished chunk 02 → extracted_outputs_1999_part02.nc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✅ Finished chunk 06 → extracted_outputs_1999_part06.nc\n",
      "✅ Finished chunk 09 → extracted_outputs_1999_part09.nc\n",
      "✅ Finished chunk 08 → extracted_outputs_1999_part08.nc\n",
      "✅ Finished chunk 07 → extracted_outputs_1999_part07.nc\n",
      "✅ Finished chunk 10 → extracted_outputs_1999_part10.nc\n",
      "✅ Finished chunk 03 → extracted_outputs_1999_part03.nc\n",
      "✅ All chunks complete.\n",
      "🔄 Merging chunks with xarray...\n",
      "✅ Merged NetCDF written to /scratch/tmiesse/project/data4trends/1999.nc and part files removed.\n",
      "✅ Finished chunk 11 → extracted_outputs_1998_part11.nc✅ Finished chunk 08 → extracted_outputs_1998_part08.nc✅ Finished chunk 03 → extracted_outputs_1998_part03.nc✅ Finished chunk 12 → extracted_outputs_1998_part12.nc✅ Finished chunk 09 → extracted_outputs_1998_part09.nc✅ Finished chunk 07 → extracted_outputs_1998_part07.nc✅ Finished chunk 10 → extracted_outputs_1998_part10.nc✅ Finished chunk 01 → extracted_outputs_1998_part01.nc✅ Finished chunk 02 → extracted_outputs_1998_part02.nc✅ Finished chunk 04 → extracted_outputs_1998_part04.nc✅ Finished chunk 05 → extracted_outputs_1998_part05.nc✅ Finished chunk 06 → extracted_outputs_1998_part06.nc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✅ All chunks complete.\n",
      "🔄 Merging chunks with xarray...\n",
      "✅ Merged NetCDF written to /scratch/tmiesse/project/data4trends/1998.nc and part files removed.\n",
      "✅ Finished chunk 10 → extracted_outputs_1997_part10.nc✅ Finished chunk 03 → extracted_outputs_1997_part03.nc✅ Finished chunk 07 → extracted_outputs_1997_part07.nc\n",
      "\n",
      "\n",
      "✅ Finished chunk 08 → extracted_outputs_1997_part08.nc✅ Finished chunk 05 → extracted_outputs_1997_part05.nc\n",
      "✅ Finished chunk 04 → extracted_outputs_1997_part04.nc\n",
      "\n",
      "✅ Finished chunk 02 → extracted_outputs_1997_part02.nc\n",
      "✅ Finished chunk 11 → extracted_outputs_1997_part11.nc\n",
      "✅ Finished chunk 12 → extracted_outputs_1997_part12.nc\n",
      "✅ Finished chunk 09 → extracted_outputs_1997_part09.nc\n",
      "✅ Finished chunk 06 → extracted_outputs_1997_part06.nc\n",
      "✅ Finished chunk 01 → extracted_outputs_1997_part01.nc\n",
      "✅ All chunks complete.\n",
      "🔄 Merging chunks with xarray...\n",
      "✅ Merged NetCDF written to /scratch/tmiesse/project/data4trends/1997.nc and part files removed.\n",
      "✅ Finished chunk 07 → extracted_outputs_1996_part07.nc✅ Finished chunk 08 → extracted_outputs_1996_part08.nc✅ Finished chunk 06 → extracted_outputs_1996_part06.nc✅ Finished chunk 04 → extracted_outputs_1996_part04.nc✅ Finished chunk 12 → extracted_outputs_1996_part12.nc✅ Finished chunk 05 → extracted_outputs_1996_part05.nc✅ Finished chunk 01 → extracted_outputs_1996_part01.nc✅ Finished chunk 10 → extracted_outputs_1996_part10.nc✅ Finished chunk 11 → extracted_outputs_1996_part11.nc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✅ Finished chunk 09 → extracted_outputs_1996_part09.nc\n",
      "✅ Finished chunk 02 → extracted_outputs_1996_part02.nc\n",
      "✅ Finished chunk 03 → extracted_outputs_1996_part03.nc\n",
      "✅ All chunks complete.\n",
      "🔄 Merging chunks with xarray...\n",
      "✅ Merged NetCDF written to /scratch/tmiesse/project/data4trends/1996.nc and part files removed.\n",
      "✅ Finished chunk 03 → extracted_outputs_1995_part03.nc✅ Finished chunk 06 → extracted_outputs_1995_part06.nc✅ Finished chunk 02 → extracted_outputs_1995_part02.nc✅ Finished chunk 12 → extracted_outputs_1995_part12.nc✅ Finished chunk 05 → extracted_outputs_1995_part05.nc✅ Finished chunk 11 → extracted_outputs_1995_part11.nc\n",
      "\n",
      "\n",
      "✅ Finished chunk 07 → extracted_outputs_1995_part07.nc✅ Finished chunk 09 → extracted_outputs_1995_part09.nc\n",
      "✅ Finished chunk 08 → extracted_outputs_1995_part08.nc\n",
      "✅ Finished chunk 10 → extracted_outputs_1995_part10.nc✅ Finished chunk 04 → extracted_outputs_1995_part04.nc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✅ Finished chunk 01 → extracted_outputs_1995_part01.nc\n",
      "✅ All chunks complete.\n",
      "🔄 Merging chunks with xarray...\n",
      "✅ Merged NetCDF written to /scratch/tmiesse/project/data4trends/1995.nc and part files removed.\n",
      "✅ Finished chunk 08 → extracted_outputs_1994_part08.nc\n",
      "✅ Finished chunk 10 → extracted_outputs_1994_part10.nc✅ Finished chunk 04 → extracted_outputs_1994_part04.nc\n",
      "✅ Finished chunk 05 → extracted_outputs_1994_part05.nc\n",
      "\n",
      "✅ Finished chunk 07 → extracted_outputs_1994_part07.nc\n",
      "✅ Finished chunk 03 → extracted_outputs_1994_part03.nc\n",
      "✅ Finished chunk 11 → extracted_outputs_1994_part11.nc\n",
      "✅ Finished chunk 12 → extracted_outputs_1994_part12.nc\n",
      "✅ Finished chunk 02 → extracted_outputs_1994_part02.nc\n",
      "✅ Finished chunk 06 → extracted_outputs_1994_part06.nc\n",
      "✅ Finished chunk 01 → extracted_outputs_1994_part01.nc\n",
      "✅ Finished chunk 09 → extracted_outputs_1994_part09.nc\n",
      "✅ All chunks complete.\n",
      "🔄 Merging chunks with xarray...\n",
      "✅ Merged NetCDF written to /scratch/tmiesse/project/data4trends/1994.nc and part files removed.\n",
      "✅ Finished chunk 10 → extracted_outputs_1993_part10.nc✅ Finished chunk 04 → extracted_outputs_1993_part04.nc✅ Finished chunk 03 → extracted_outputs_1993_part03.nc✅ Finished chunk 02 → extracted_outputs_1993_part02.nc✅ Finished chunk 11 → extracted_outputs_1993_part11.nc✅ Finished chunk 12 → extracted_outputs_1993_part12.nc✅ Finished chunk 07 → extracted_outputs_1993_part07.nc✅ Finished chunk 01 → extracted_outputs_1993_part01.nc✅ Finished chunk 05 → extracted_outputs_1993_part05.nc✅ Finished chunk 06 → extracted_outputs_1993_part06.nc✅ Finished chunk 08 → extracted_outputs_1993_part08.nc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✅ Finished chunk 09 → extracted_outputs_1993_part09.nc\n",
      "✅ All chunks complete.\n",
      "🔄 Merging chunks with xarray...\n",
      "✅ Merged NetCDF written to /scratch/tmiesse/project/data4trends/1993.nc and part files removed.\n",
      "✅ Finished chunk 08 → extracted_outputs_1992_part08.nc✅ Finished chunk 10 → extracted_outputs_1992_part10.nc✅ Finished chunk 09 → extracted_outputs_1992_part09.nc✅ Finished chunk 05 → extracted_outputs_1992_part05.nc✅ Finished chunk 12 → extracted_outputs_1992_part12.nc✅ Finished chunk 04 → extracted_outputs_1992_part04.nc✅ Finished chunk 07 → extracted_outputs_1992_part07.nc✅ Finished chunk 03 → extracted_outputs_1992_part03.nc✅ Finished chunk 06 → extracted_outputs_1992_part06.nc✅ Finished chunk 01 → extracted_outputs_1992_part01.nc✅ Finished chunk 11 → extracted_outputs_1992_part11.nc✅ Finished chunk 02 → extracted_outputs_1992_part02.nc\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "✅ All chunks complete.\n",
      "🔄 Merging chunks with xarray...\n",
      "✅ Merged NetCDF written to /scratch/tmiesse/project/data4trends/1992.nc and part files removed.\n"
     ]
    }
   ],
   "source": [
    "YEARS = [2000,1999,1998,1997,1996,1995,1994,1993,1992,1991,1990]\n",
    "for YEAR in YEARS:\n",
    "    N_CHUNKS = 12\n",
    "    ROOT = pl.Path('/groups/ORC-CLIMATE/fhrl_repo/Arctic_Database/Raw_DATA')\n",
    "    PROCESSED_DIR = pl.Path('/scratch/tmiesse/project/data4trends')\n",
    "    SHAPEFILE = pl.Path('/groups/ORC-CLIMATE/fhrl_repo/Arctic_Database/arctic_shapefiles/comm4process/stations_locs2.shp')\n",
    "    OUTPUT_DIR = ROOT / str(YEAR) / \"outputs\"\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        gdf = gpd.read_file(SHAPEFILE)\n",
    "        chunks = np.array_split(gdf, N_CHUNKS)\n",
    "        args = [(chunk.reset_index(drop=True), i+1) for i, chunk in enumerate(chunks)]\n",
    "        with mp.Pool(N_CHUNKS) as pool:\n",
    "            pool.starmap(process_chunk, args)\n",
    "        print(\"✅ All chunks complete.\")\n",
    "\n",
    "    part_files = sorted(PROCESSED_DIR.glob(f\"extracted_outputs_{YEAR}_part*.nc\"))\n",
    "    if part_files:\n",
    "        print(\"🔄 Merging chunks with xarray...\")\n",
    "        ds_merged = xr.open_mfdataset(part_files, combine='nested', concat_dim='node')\n",
    "        merged_path = PROCESSED_DIR / f\"{YEAR}.nc\"\n",
    "        ds_merged.to_netcdf(merged_path)\n",
    "        ds_merged.close()\n",
    "\n",
    "        # Cleanup\n",
    "        for f in part_files:\n",
    "            os.remove(f)\n",
    "        print(f\"✅ Merged NetCDF written to {merged_path} and part files removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
